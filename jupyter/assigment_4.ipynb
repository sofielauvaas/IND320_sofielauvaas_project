{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IND320 - Data to Decision\n",
    "**Student:** Sofie LauvÃ¥s\n",
    "\n",
    "**Project:** Data Update and Consumption Data Acquisition\n",
    "\n",
    "**GitHub Repository:** https://github.com/sofielauvaas/IND320_sofielauvaas_project\n",
    "\n",
    "**Streamlit App:** https://ind320sofielauvaasproject.streamlit.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import tomllib\n",
    "import time\n",
    "\n",
    "# Database and Spark Imports\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from cassandra.cluster import Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spark and Cassandra Connection Test\n",
    "\n",
    "#### 2.1 Spart set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:02:47 WARN Utils: Your hostname, Sofies-Mac-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "25/11/26 09:02:47 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Ivy Default Cache set to: /Users/sofiesveindal/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sofiesveindal/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-48832ac0-7a7a-43aa-8ce0-34c1b9ae8e03;1.0\n",
      "\tconfs: [default]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/miniconda3/envs/D2D_env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 354ms :: artifacts dl 13ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-48832ac0-7a7a-43aa-8ce0-34c1b9ae8e03\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/7ms)\n",
      "25/11/26 09:02:48 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320_A4_Data_Ingestion\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark session started successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cassandra driver & Keyspace set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Cassandra keyspace: ind320_project\n"
     ]
    }
   ],
   "source": [
    "keyspace_name = \"ind320_project\"\n",
    "test_table_name = \"test_spark_connection\"\n",
    "\n",
    "# Connect to Cassandra using the PyCassandra driver\n",
    "cluster = Cluster([\"127.0.0.1\"])\n",
    "session = cluster.connect()\n",
    "\n",
    "# Ensure the keyspace exists (idempotent operation)\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {keyspace_name}\n",
    "WITH replication = {{'class':'SimpleStrategy', 'replication_factor' : 1}};\n",
    "\"\"\")\n",
    "session.set_keyspace(keyspace_name)\n",
    "print(f\"Connected to Cassandra keyspace: {keyspace_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Spark - Cassandra pipeline verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Data Read by Spark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|  1|verification_success|\n",
      "+---+--------------------+\n",
      "\n",
      "Cleaned up temporary table: test_spark_connection\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table for verification\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {test_table_name} (\n",
    "    id int PRIMARY KEY,\n",
    "    name text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data using the Python driver\n",
    "session.execute(f\"INSERT INTO {test_table_name} (id, name) VALUES (1, 'verification_success');\")\n",
    "\n",
    "# Read data back using the Spark connector (The critical test for the full pipeline)\n",
    "df_test = (\n",
    "    spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=keyspace_name, table=test_table_name)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Verification Data Read by Spark:\")\n",
    "df_test.show()\n",
    "\n",
    "# Cleanup: Drop the temporary table to keep the keyspace clean for A4 tables\n",
    "session.execute(f\"DROP TABLE {test_table_name};\")\n",
    "print(f\"Cleaned up temporary table: {test_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration and API Utility Functions\n",
    "\n",
    "#### 3.1 Load Secrets and Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants based on the Assignment 4 requirements\n",
    "PRODUCTION_DATASET = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "CONSUMPTION_DATASET = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "ENTITY = \"price-areas\" # The API entity used for the Elhub.no API\n",
    "\n",
    "# Years required by the assignment\n",
    "# Production requires 2022, 2023, 2024 (new data to append)\n",
    "PRODUCTION_YEARS = [2022, 2023, 2024] \n",
    "# Consumption requires 2021, 2022, 2023, 2024 (full dataset)\n",
    "CONSUMPTION_YEARS = [2021, 2022, 2023, 2024] \n",
    "\n",
    "\n",
    "# Read secrets from secrets.toml (assuming it is in the project root or accessible)\n",
    "with open(\"../.streamlit/secrets.toml\", \"rb\") as f:\n",
    "    secrets = tomllib.load(f)\n",
    "\n",
    "# Store MongoDB URI\n",
    "MONGO_URI = secrets[\"mongodb\"][\"uri\"]\n",
    "MONGO_DB = secrets[\"mongodb\"][\"database\"]\n",
    "MONGO_COLLECTION = secrets[\"mongodb\"][\"collection\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 API Fetcher Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_ranges(year):\n",
    "    \"\"\"Generates monthly date ranges for a given year in the required API format.\"\"\"\n",
    "    monthly_ranges = []\n",
    "    for month in range(1, 13):\n",
    "        start = datetime(year, month, 1, 0, 0, 0)\n",
    "        last_day = calendar.monthrange(year, month)[1]\n",
    "        end = datetime(year, month, last_day, 23, 59, 59)\n",
    "        \n",
    "        # Format datetime strings for API, including the timezone offset (%2B01)\n",
    "        start_str = start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01\"\n",
    "        end_str = end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01\"\n",
    "        \n",
    "        monthly_ranges.append((start_str, end_str))\n",
    "    return monthly_ranges\n",
    "\n",
    "def fetch_data_from_elhub(dataset_name: str, years: list):\n",
    "    \"\"\"\n",
    "    Fetches hourly data from the Elhub API, converts column names from camelCase \n",
    "    to snake_case, and returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting fetch for Dataset: {dataset_name} (Years: {years}) ---\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Base URL for the Norwegian Elhub API\n",
    "    BASE_URL = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={dataset_name}&startDate={{}}&endDate={{}}\"\n",
    "    \n",
    "    # Determine the expected key in the JSON response (e.g., 'consumptionPerGroupMbaHour')\n",
    "    # Convert the snake_case constant to camelCase to find the nested list in the response\n",
    "    list_key = \"\".join(word.capitalize() for word in dataset_name.lower().split('_'))\n",
    "    list_key = list_key[0].lower() + list_key[1:]\n",
    "    \n",
    "    print(f\"Expecting data under JSON key: '{list_key}'\")\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Fetching data for year {year}...\")\n",
    "        monthly_ranges = get_monthly_ranges(year)\n",
    "        \n",
    "        for start_date, end_date in monthly_ranges:\n",
    "            url = BASE_URL.format(start_date, end_date)\n",
    "            \n",
    "            # --- CRITICAL CHANGE: NO API KEY HEADER USED HERE ---\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract the list from the deep, nested JSON structure \n",
    "                for entry in data.get(\"data\", []):\n",
    "                    \n",
    "                    monthly_list = entry.get(\"attributes\", {}).get(list_key, [])\n",
    "                    \n",
    "                    # Add data to the main list and standardize to have the 'dataset' for tracking\n",
    "                    for record in monthly_list:\n",
    "                         record['dataset'] = dataset_name\n",
    "                         all_data.append(record)\n",
    "\n",
    "            else:\n",
    "                print(f\"  -> Failed to retrieve data for {start_date} to {end_date}: HTTP {response.status_code}\")\n",
    "                \n",
    "        print(f\"  -> Finished processing {year}.\")\n",
    "\n",
    "    if all_data:\n",
    "        # Define the mapping from camelCase (API fields) to snake_case (desired schema)\n",
    "        column_mapping = {\n",
    "            'priceArea': 'pricearea',\n",
    "            'productionGroup': 'productiongroup',\n",
    "            'startTime': 'starttime',\n",
    "            'endTime': 'endtime',\n",
    "            'quantityKwh': 'quantitykwh',\n",
    "            'lastUpdatedTime': 'lastupdatedtime'\n",
    "        }\n",
    "        \n",
    "        # Convert the collected list of dictionaries to a Pandas DataFrame\n",
    "        final_df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Rename columns directly to snake_case for consistency (This is the key change)\n",
    "        # The 'dataset' column, added earlier, remains as is.\n",
    "        final_df.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        print(f\"\\nTotal data points fetched for {dataset_name}: {len(final_df)}\")\n",
    "        return final_df\n",
    "        \n",
    "    print(f\"\\nNo data successfully fetched for {dataset_name}.\")\n",
    "    return pd.DataFrame()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting fetch for Dataset: PRODUCTION_PER_GROUP_MBA_HOUR (Years: [2022, 2023, 2024]) ---\n",
      "Expecting data under JSON key: 'productionPerGroupMbaHour'\n",
      "Fetching data for year 2022...\n",
      "  -> Finished processing 2022.\n",
      "Fetching data for year 2023...\n",
      "  -> Finished processing 2023.\n",
      "Fetching data for year 2024...\n",
      "  -> Finished processing 2024.\n",
      "\n",
      "Total data points fetched for PRODUCTION_PER_GROUP_MBA_HOUR: 657600\n",
      "\n",
      "--- Starting fetch for Dataset: CONSUMPTION_PER_GROUP_MBA_HOUR (Years: [2021, 2022, 2023, 2024]) ---\n",
      "Expecting data under JSON key: 'consumptionPerGroupMbaHour'\n",
      "Fetching data for year 2021...\n",
      "  -> Finished processing 2021.\n",
      "Fetching data for year 2022...\n",
      "  -> Finished processing 2022.\n",
      "Fetching data for year 2023...\n",
      "  -> Finished processing 2023.\n",
      "Fetching data for year 2024...\n",
      "  -> Finished processing 2024.\n",
      "\n",
      "Total data points fetched for CONSUMPTION_PER_GROUP_MBA_HOUR: 876600\n",
      "\n",
      "Production Data Head:\n",
      "                     endtime            lastupdatedtime pricearea  \\\n",
      "0  2022-01-01T01:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "1  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "2  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "3  2022-01-01T04:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "4  2022-01-01T05:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "\n",
      "  productiongroup  quantitykwh                  starttime  \\\n",
      "0           hydro    1291422.4  2022-01-01T00:00:00+01:00   \n",
      "1           hydro    1246209.4  2022-01-01T01:00:00+01:00   \n",
      "2           hydro    1271757.0  2022-01-01T02:00:00+01:00   \n",
      "3           hydro    1204251.8  2022-01-01T03:00:00+01:00   \n",
      "4           hydro    1202086.9  2022-01-01T04:00:00+01:00   \n",
      "\n",
      "                         dataset  \n",
      "0  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "1  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "2  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "3  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "4  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "\n",
      "Consumption Data Head:\n",
      "  consumptionGroup                    endtime            lastupdatedtime  \\\n",
      "0            cabin  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "1            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "2            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "3            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "4            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "\n",
      "   meteringPointCount pricearea  quantitykwh                  starttime  \\\n",
      "0              100607       NO1    177071.56  2021-01-01T00:00:00+01:00   \n",
      "1              100607       NO1    171335.12  2021-01-01T01:00:00+01:00   \n",
      "2              100607       NO1    164912.02  2021-01-01T02:00:00+01:00   \n",
      "3              100607       NO1    160265.77  2021-01-01T03:00:00+01:00   \n",
      "4              100607       NO1    159828.69  2021-01-01T04:00:00+01:00   \n",
      "\n",
      "                          dataset  \n",
      "0  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "1  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "2  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "3  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "4  CONSUMPTION_PER_GROUP_MBA_HOUR  \n"
     ]
    }
   ],
   "source": [
    "# Retrieve Production Data (2022-2024)\n",
    "production_df_raw = fetch_data_from_elhub(\n",
    "    dataset_name=PRODUCTION_DATASET,\n",
    "    years=PRODUCTION_YEARS\n",
    ")\n",
    "\n",
    "# Retrieve Consumption Data (2021-2024)\n",
    "consumption_df_raw = fetch_data_from_elhub(\n",
    "    dataset_name=CONSUMPTION_DATASET,\n",
    "    years=CONSUMPTION_YEARS\n",
    ")\n",
    "\n",
    "print(\"\\nProduction Data Head:\")\n",
    "print(production_df_raw.head())\n",
    "print(\"\\nConsumption Data Head:\")\n",
    "print(consumption_df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Restart/Refresh Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Refreshing Spark Session for Data Transformation ---\n",
      "Previous Spark session stopped.\n",
      "New Spark session started successfully for transformation.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Refreshing Spark Session for Data Transformation ---\")\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Previous Spark session stopped.\")\n",
    "except:\n",
    "    pass # Ignore if it was already stopped\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320_A4_Data_Ingestion\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"New Spark session started successfully for transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spark Transformation and Cassandra Ingestion\n",
    "\n",
    "#### 4.1 Spark Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Could not read 2021 data from 'production_2021'. Using only new data.\n",
      "Spark DataFrames created for production (full 2021-2024) and consumption (2021-2024).\n",
      "\n",
      "Production Full Spark DF Schema:\n",
      "root\n",
      " |-- productiongroup: string (nullable = true)\n",
      " |-- pricearea: string (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- endtime: timestamp (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      " |-- lastupdatedtime: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Consumption Spark DF Schema:\n",
      "root\n",
      " |-- pricearea: string (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- endtime: timestamp (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      " |-- lastupdatedtime: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#### 4.1 Spark Transformation Function and Data Union\n",
    "def transform_pandas_to_spark_df(pandas_df, spark_session, data_type):\n",
    "    \"\"\"\n",
    "    Converts Pandas (which is already snake_case from the fetch step) to Spark,\n",
    "    and casts time columns to Spark Timestamp type (CRITICAL for Cassandra storage).\n",
    "    \"\"\"\n",
    "    if pandas_df.empty:\n",
    "        print(f\"Warning: Empty DataFrame provided for transformation ({data_type}). Returning empty Spark DF.\")\n",
    "        return spark_session.createDataFrame([], schema=None)\n",
    "        \n",
    "    spark_df = spark_session.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Convert time columns using the timezone-aware format string\n",
    "    time_columns = ['starttime', 'endtime', 'lastupdatedtime']\n",
    "    for col_name in time_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "             spark_df = spark_df.withColumn(\n",
    "                 col_name, \n",
    "                 to_timestamp(col(col_name), \"yyyy-MM-dd'T'HH:mm:ssXXX\") \n",
    "             )\n",
    "    \n",
    "    # Select final columns based on data type for the respective Cassandra tables\n",
    "    base_cols = ['pricearea', 'starttime', 'endtime', 'quantitykwh', 'lastupdatedtime']\n",
    "    \n",
    "    if data_type == 'production':\n",
    "        final_cols = ['productiongroup'] + base_cols\n",
    "    else: # Consumption data\n",
    "        final_cols = base_cols\n",
    "        \n",
    "    # Filter columns to only include those present in the DataFrame\n",
    "    return spark_df.select([c for c in final_cols if c in spark_df.columns])\n",
    "\n",
    "# 1. Transform the raw dataframes (New data: 2022-2024 Production, 2021-2024 Consumption)\n",
    "production_spark_df_new = transform_pandas_to_spark_df(production_df_raw, spark, 'production')\n",
    "consumption_spark_df = transform_pandas_to_spark_df(consumption_df_raw, spark, 'consumption')\n",
    "\n",
    "\n",
    "# 2. CONSOLIDATION: Read the old 2021 Production data from the staging table\n",
    "old_production_table = \"production_2021\"\n",
    "try:\n",
    "    production_spark_df_2021 = (\n",
    "        spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "        .options(keyspace=keyspace_name, table=old_production_table)\n",
    "        .load()\n",
    "    )\n",
    "    # Ensure columns match for union (e.g., lowercase names)\n",
    "    production_spark_df_2021 = production_spark_df_2021.select(production_spark_df_new.columns)\n",
    "    print(f\"\\nRead {production_spark_df_2021.count()} 2021 records from '{old_production_table}'.\")\n",
    "\n",
    "    # 3. UNION: Combine the old 2021 data with the new 2022-2024 data\n",
    "    production_spark_df_full = production_spark_df_2021.union(production_spark_df_new)\n",
    "    print(f\"Total Combined Production Records (2021-2024): {production_spark_df_full.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # If the old table doesn't exist, assume we're only writing the new data (this shouldn't happen)\n",
    "    print(f\"\\nWarning: Could not read 2021 data from '{old_production_table}'. Using only new data.\")\n",
    "    production_spark_df_full = production_spark_df_new\n",
    "\n",
    "print(\"Spark DataFrames created for production (full 2021-2024) and consumption (2021-2024).\")\n",
    "print(\"\\nProduction Full Spark DF Schema:\")\n",
    "production_spark_df_full.printSchema()\n",
    "print(\"\\nConsumption Spark DF Schema:\")\n",
    "consumption_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Cassandra Table Creation (New/Updated Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Cassandra tables 'production_hourly' and 'consumption_hourly' explicitly dropped and re-created with the correct schema.\n"
     ]
    }
   ],
   "source": [
    "#### 4.2 Cassandra Table Creation (New/Updated Tables)\n",
    "production_table_name = \"production_hourly\"\n",
    "consumption_table_name = \"consumption_hourly\"\n",
    "\n",
    "# --- CRITICAL FIX: Explicitly drop old tables to ensure the new schema is used ---\n",
    "# If a previous run created these tables with snake_case column names,\n",
    "# we must drop them to successfully re-create them with the new lowercase names.\n",
    "\n",
    "# 1. Drop the production table (including the old production_2021 table if it still exists)\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.production_2021;\")\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.{production_table_name};\")\n",
    "\n",
    "# 2. Drop the consumption table\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.{consumption_table_name};\")\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 3. Production Table: Re-create with the all-lowercase schema\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {keyspace_name}.{production_table_name} (\n",
    "    pricearea text,\n",
    "    productiongroup text,\n",
    "    starttime timestamp,\n",
    "    endtime timestamp,\n",
    "    quantitykwh double,\n",
    "    lastupdatedtime timestamp,\n",
    "    PRIMARY KEY ((pricearea, productiongroup), starttime)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 4. Consumption Table: Re-create with the all-lowercase schema\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {keyspace_name}.{consumption_table_name} (\n",
    "    pricearea text,\n",
    "    starttime timestamp,\n",
    "    endtime timestamp,\n",
    "    quantitykwh double,\n",
    "    lastupdatedtime timestamp,\n",
    "    PRIMARY KEY (pricearea, starttime)\n",
    ");\n",
    "\"\"\")\n",
    "print(f\"Target Cassandra tables '{production_table_name}' and '{consumption_table_name}' explicitly dropped and re-created with the correct schema.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Ingestion into Cassandra (Append vs. New Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:05:10 WARN TaskSetManager: Stage 0 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/26 09:05:15 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "25/11/26 09:05:15 WARN TaskSetManager: Stage 1 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/26 09:05:31 WARN TaskSetManager: Stage 2 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/26 09:05:34 WARN TaskSetManager: Stage 5 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully OVERWROTE 657600 records (2021-2024) into 'production_hourly'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:05:38 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 18): Attempting to kill Python Worker\n",
      "25/11/26 09:05:38 WARN TaskSetManager: Stage 6 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/26 09:05:51 WARN TaskSetManager: Stage 7 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 7:=======>                                                   (1 + 7) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully INSERTED 876600 records (2021-2024) into new table 'consumption_hourly'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#### 4.3 Ingestion into Cassandra (Full Dataset Write)\n",
    "# Ingest Production Data (2021-2024 Full Dataset) - OVERWRITING\n",
    "# We use 'overwrite' because the full, combined dataset is ready and the table was just cleared.\n",
    "if 'production_spark_df_full' in locals() and not production_spark_df_full.isEmpty():\n",
    "    # *** CRITICAL FIX: Add 'confirm.truncate' option to allow overwrite mode ***\n",
    "    production_spark_df_full.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace_name, table=production_table_name, **{'confirm.truncate': True}) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(f\"\\nSuccessfully OVERWROTE {production_spark_df_full.count()} records (2021-2024) into '{production_table_name}'.\")\n",
    "else:\n",
    "    print(f\"\\nSkipped Production Ingestion: No 2021-2024 production data to insert.\")\n",
    "\n",
    "\n",
    "# Ingest Consumption Data (2021-2024) - INSERTING (New table)\n",
    "# The consumption_spark_df contains the full 2021-2024 dataset, so we just write it.\n",
    "if not consumption_spark_df.isEmpty():\n",
    "    consumption_spark_df.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace_name, table=consumption_table_name) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    print(f\"Successfully INSERTED {consumption_spark_df.count()} records (2021-2024) into new table '{consumption_table_name}'.\")\n",
    "else:\n",
    "    print(f\"Skipped Consumption Ingestion: No 2021-2024 consumption data to insert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MongoDB Ingestion and Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:05:53 WARN TaskSetManager: Stage 10 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- CRITICAL CHECK: Production DF Columns ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:05:57 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 10 (TID 36): Attempting to kill Python Worker\n",
      "25/11/26 09:05:57 WARN TaskSetManager: Stage 11 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production DF Columns (Target for MongoDB): ['productiongroup', 'pricearea', 'starttime', 'endtime', 'quantitykwh', 'lastupdatedtime']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:05:58 WARN TaskSetManager: Stage 14 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Production DF Count: 657600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:06:02 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 14 (TID 46): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to MongoDB collection 'production'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/26 09:06:03 WARN TaskSetManager: Stage 15 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing documents in MongoDB collection.\n",
      "\n",
      "Inserted 657600 Production records (2021-2024).\n",
      "Total documents in collection: 657600\n",
      "\n",
      "Sample document from MongoDB:\n",
      "{'_id': ObjectId('6926b5a28e7dfcb511231052'), 'productiongroup': 'hydro', 'pricearea': 'NO1', 'starttime': datetime.datetime(2022, 1, 1, 0, 0), 'endtime': datetime.datetime(2022, 1, 1, 1, 0), 'quantitykwh': 1291422.4, 'lastupdatedtime': datetime.datetime(2025, 2, 1, 18, 2, 57)}\n",
      "\n",
      "Spark session stopped. All ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "### 5. MongoDB Ingestion and Final Cleanup\n",
    "\n",
    "# --- CRITICAL CHECK: Columns of Production Data (what Streamlit needs) ---\n",
    "# This block confirms that the DataFrame we are about to insert has the necessary column.\n",
    "print(\"\\n--- CRITICAL CHECK: Production DF Columns ---\")\n",
    "# Check if the full production DF exists and is not empty\n",
    "if 'production_spark_df_full' in locals() and not production_spark_df_full.isEmpty():\n",
    "    print(\"Production DF Columns (Target for MongoDB):\", production_spark_df_full.columns)\n",
    "    print(\"Production DF Count:\", production_spark_df_full.count())\n",
    "    mongo_target_df = production_spark_df_full\n",
    "    data_type_label = \"Production\"\n",
    "else:\n",
    "    print(\"ERROR: production_spark_df_full is empty or missing. Cannot insert data needed for Production page.\")\n",
    "    mongo_target_df = spark.createDataFrame([], schema=None) # Empty DF to safely skip ingestion\n",
    "    data_type_label = \"Error_Empty\"\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# Requirement: Load the full PRODUCTION data (2021-2024) into the main MongoDB collection.\n",
    "# This ensures the Streamlit Production page has the correct dataset.\n",
    "if MONGO_URI and not mongo_target_df.isEmpty():\n",
    "    \n",
    "    # 5.1 Connect to MongoDB\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB]\n",
    "    collection = db[MONGO_COLLECTION]\n",
    "    print(f\"\\nConnected to MongoDB collection '{MONGO_COLLECTION}'.\")\n",
    "\n",
    "    # 5.2 Convert Spark DataFrame back to Pandas for MongoDB insertion\n",
    "    # We use the full Production DF which includes 'productiongroup'\n",
    "    mongo_pandas_df = mongo_target_df.toPandas()\n",
    "    \n",
    "    # Convert time columns to Python datetime objects for BSON compliance\n",
    "    for col_name in ['starttime', 'endtime', 'lastupdatedtime']:\n",
    "        if col_name in mongo_pandas_df.columns:\n",
    "            # Use .apply() to safely convert timestamps to datetime objects\n",
    "            mongo_pandas_df[col_name] = mongo_pandas_df[col_name].apply(\n",
    "                lambda x: x.to_pydatetime() if pd.notna(x) else None\n",
    "            )\n",
    "            \n",
    "    # Convert to a list of dictionaries (BSON documents)\n",
    "    records = mongo_pandas_df.to_dict(\"records\")\n",
    "\n",
    "    # 5.3 Clear existing data and insert the new Production data\n",
    "    collection.delete_many({})\n",
    "    print(\"Cleared existing documents in MongoDB collection.\")\n",
    "    \n",
    "    # Insert the new, comprehensive Production data\n",
    "    result = collection.insert_many(records)\n",
    "\n",
    "    print(f\"\\nInserted {len(result.inserted_ids)} {data_type_label} records (2021-2024).\")\n",
    "    print(f\"Total documents in collection: {collection.count_documents({})}\")\n",
    "    print(\"\\nSample document from MongoDB:\")\n",
    "    print(collection.find_one())\n",
    "    \n",
    "else:\n",
    "    print(\"\\nMongoDB ingestion skipped (URI not loaded or target data is empty).\")\n",
    "    \n",
    "### 6. Final Cleanup\n",
    "\n",
    "# Stop Spark Session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"\\nSpark session stopped. All ingestion complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nSpark session stop failed (was likely already stopped): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Usage"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
