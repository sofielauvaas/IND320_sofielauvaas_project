{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IND320 - Data to Decision\n",
    "**Student:** Sofie Lauvås\n",
    "\n",
    "**Project:** Machine Learning\n",
    "\n",
    "**GitHub Repository:** https://github.com/sofielauvaas/IND320_sofielauvaas_project\n",
    "\n",
    "**Streamlit App:** https://ind320sofielauvaasproject.streamlit.app/\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Project Setup and Library Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import calendar\n",
    "import tomllib\n",
    "import time\n",
    "\n",
    "# Database and Spark Imports\n",
    "from cassandra.cluster import Cluster\n",
    "from pymongo import MongoClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import to_timestamp, col\n",
    "import time\n",
    "from pyspark.sql import SparkSession\n",
    "from cassandra.cluster import Cluster"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Spark and Cassandra Connection Test\n",
    "\n",
    "#### 2.1 Spark set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:33:08 WARN Utils: Your hostname, Sofies-Mac-Air.local resolves to a loopback address: 127.0.0.1; using 192.168.1.3 instead (on interface en0)\n",
      "25/11/27 13:33:08 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/opt/miniconda3/envs/D2D_env/lib/python3.12/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/sofiesveindal/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/sofiesveindal/.ivy2/jars\n",
      "com.datastax.spark#spark-cassandra-connector_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5ce67939-76e9-4550-8510-1c8f24fae861;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.datastax.spark#spark-cassandra-connector_2.12;3.5.1 in central\n",
      "\tfound com.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 in central\n",
      "\tfound org.scala-lang.modules#scala-collection-compat_2.12;2.11.0 in central\n",
      "\tfound org.apache.cassandra#java-driver-core-shaded;4.18.1 in central\n",
      "\tfound com.datastax.oss#native-protocol;1.5.1 in central\n",
      "\tfound com.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 in central\n",
      "\tfound com.typesafe#config;1.4.1 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.26 in central\n",
      "\tfound io.dropwizard.metrics#metrics-core;4.1.18 in central\n",
      "\tfound org.hdrhistogram#HdrHistogram;2.1.12 in central\n",
      "\tfound org.reactivestreams#reactive-streams;1.0.3 in central\n",
      "\tfound org.apache.cassandra#java-driver-mapper-runtime;4.18.1 in central\n",
      "\tfound org.apache.cassandra#java-driver-query-builder;4.18.1 in central\n",
      "\tfound org.apache.commons#commons-lang3;3.10 in central\n",
      "\tfound com.thoughtworks.paranamer#paranamer;2.8 in central\n",
      "\tfound org.scala-lang#scala-reflect;2.12.19 in central\n",
      ":: resolution report :: resolve 2504ms :: artifacts dl 156ms\n",
      "\t:: modules in use:\n",
      "\tcom.datastax.oss#java-driver-shaded-guava;25.1-jre-graal-sub-1 from central in [default]\n",
      "\tcom.datastax.oss#native-protocol;1.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector-driver_2.12;3.5.1 from central in [default]\n",
      "\tcom.datastax.spark#spark-cassandra-connector_2.12;3.5.1 from central in [default]\n",
      "\tcom.thoughtworks.paranamer#paranamer;2.8 from central in [default]\n",
      "\tcom.typesafe#config;1.4.1 from central in [default]\n",
      "\tio.dropwizard.metrics#metrics-core;4.1.18 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-core-shaded;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-mapper-runtime;4.18.1 from central in [default]\n",
      "\torg.apache.cassandra#java-driver-query-builder;4.18.1 from central in [default]\n",
      "\torg.apache.commons#commons-lang3;3.10 from central in [default]\n",
      "\torg.hdrhistogram#HdrHistogram;2.1.12 from central in [default]\n",
      "\torg.reactivestreams#reactive-streams;1.0.3 from central in [default]\n",
      "\torg.scala-lang#scala-reflect;2.12.19 from central in [default]\n",
      "\torg.scala-lang.modules#scala-collection-compat_2.12;2.11.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.26 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   16  |   0   |   0   |   0   ||   16  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5ce67939-76e9-4550-8510-1c8f24fae861\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 16 already retrieved (0kB/20ms)\n",
      "25/11/27 13:33:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark session started successfully.\n"
     ]
    }
   ],
   "source": [
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320_A4_Data_Ingestion\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"Spark session started successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 Cassandra driver & Keyspace set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to Cassandra keyspace: ind320_project\n"
     ]
    }
   ],
   "source": [
    "keyspace_name = \"ind320_project\"\n",
    "test_table_name = \"test_spark_connection\"\n",
    "\n",
    "# Connect to Cassandra using the PyCassandra driver\n",
    "cluster = Cluster([\"127.0.0.1\"])\n",
    "session = cluster.connect()\n",
    "\n",
    "# Ensure the keyspace exists (idempotent operation)\n",
    "session.execute(f\"\"\"\n",
    "CREATE KEYSPACE IF NOT EXISTS {keyspace_name}\n",
    "WITH replication = {{'class':'SimpleStrategy', 'replication_factor' : 1}};\n",
    "\"\"\")\n",
    "session.set_keyspace(keyspace_name)\n",
    "print(f\"Connected to Cassandra keyspace: {keyspace_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 Spark - Cassandra pipeline verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Verification Data Read by Spark:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+--------------------+\n",
      "| id|                name|\n",
      "+---+--------------------+\n",
      "|  1|verification_success|\n",
      "+---+--------------------+\n",
      "\n",
      "Cleaned up temporary table: test_spark_connection\n"
     ]
    }
   ],
   "source": [
    "# Create a temporary table for verification\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {test_table_name} (\n",
    "    id int PRIMARY KEY,\n",
    "    name text\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# Insert data using the Python driver\n",
    "session.execute(f\"INSERT INTO {test_table_name} (id, name) VALUES (1, 'verification_success');\")\n",
    "\n",
    "# Read data back using the Spark connector (The critical test for the full pipeline)\n",
    "df_test = (\n",
    "    spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "    .options(keyspace=keyspace_name, table=test_table_name)\n",
    "    .load()\n",
    ")\n",
    "\n",
    "print(\"Verification Data Read by Spark:\")\n",
    "df_test.show()\n",
    "\n",
    "# Cleanup: Drop the temporary table to keep the keyspace clean for A4 tables\n",
    "session.execute(f\"DROP TABLE {test_table_name};\")\n",
    "print(f\"Cleaned up temporary table: {test_table_name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Configuration and API Utility Functions\n",
    "\n",
    "#### 3.1 Load Secrets and Define Constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants based on the Assignment 4 requirements\n",
    "PRODUCTION_DATASET = \"PRODUCTION_PER_GROUP_MBA_HOUR\"\n",
    "CONSUMPTION_DATASET = \"CONSUMPTION_PER_GROUP_MBA_HOUR\"\n",
    "ENTITY = \"price-areas\" # The API entity used for the Elhub.no API\n",
    "\n",
    "# Years required by the assignment\n",
    "# Production requires 2022, 2023, 2024 (new data to append)\n",
    "PRODUCTION_YEARS = [2022, 2023, 2024] \n",
    "# Consumption requires 2021, 2022, 2023, 2024 (full dataset)\n",
    "CONSUMPTION_YEARS = [2021, 2022, 2023, 2024] \n",
    "\n",
    "\n",
    "# Read secrets from secrets.toml (assuming it is in the project root or accessible)\n",
    "with open(\"../.streamlit/secrets.toml\", \"rb\") as f:\n",
    "    secrets = tomllib.load(f)\n",
    "\n",
    "# Store MongoDB URI\n",
    "MONGO_URI = secrets[\"mongodb\"][\"uri\"]\n",
    "MONGO_DB = secrets[\"mongodb\"][\"database\"]\n",
    "MONGO_COLLECTION = secrets[\"mongodb\"][\"collection\"]\n",
    "MONGO_CONSUMPTION_COLLECTION = \"consumption_hourly\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.2 API Fetcher Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_monthly_ranges(year):\n",
    "    \"\"\"Generates monthly date ranges for a given year in the required API format.\"\"\"\n",
    "    monthly_ranges = []\n",
    "    for month in range(1, 13):\n",
    "        start = datetime(year, month, 1, 0, 0, 0)\n",
    "        last_day = calendar.monthrange(year, month)[1]\n",
    "        end = datetime(year, month, last_day, 23, 59, 59)\n",
    "        \n",
    "        # Format datetime strings for API, including the timezone offset (%2B01)\n",
    "        start_str = start.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01\"\n",
    "        end_str = end.strftime(\"%Y-%m-%dT%H:%M:%S\") + \"%2B01\"\n",
    "        \n",
    "        monthly_ranges.append((start_str, end_str))\n",
    "    return monthly_ranges\n",
    "\n",
    "def fetch_data_from_elhub(dataset_name: str, years: list):\n",
    "    \"\"\"\n",
    "    Fetches hourly data from the Elhub API, converts column names from camelCase \n",
    "    to snake_case, and returns a Pandas DataFrame.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Starting fetch for Dataset: {dataset_name} (Years: {years}) ---\")\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    # Base URL for the Norwegian Elhub API\n",
    "    BASE_URL = f\"https://api.elhub.no/energy-data/v0/{ENTITY}?dataset={dataset_name}&startDate={{}}&endDate={{}}\"\n",
    "    \n",
    "    # Determine the expected key in the JSON response (e.g., 'consumptionPerGroupMbaHour')\n",
    "    # Convert the snake_case constant to camelCase to find the nested list in the response\n",
    "    list_key = \"\".join(word.capitalize() for word in dataset_name.lower().split('_'))\n",
    "    list_key = list_key[0].lower() + list_key[1:]\n",
    "    \n",
    "    print(f\"Expecting data under JSON key: '{list_key}'\")\n",
    "\n",
    "    for year in years:\n",
    "        print(f\"Fetching data for year {year}...\")\n",
    "        monthly_ranges = get_monthly_ranges(year)\n",
    "        \n",
    "        for start_date, end_date in monthly_ranges:\n",
    "            url = BASE_URL.format(start_date, end_date)\n",
    "            \n",
    "            # --- CRITICAL CHANGE: NO API KEY HEADER USED HERE ---\n",
    "            response = requests.get(url)\n",
    "            \n",
    "            if response.status_code == 200:\n",
    "                data = response.json()\n",
    "                \n",
    "                # Extract the list from the deep, nested JSON structure \n",
    "                for entry in data.get(\"data\", []):\n",
    "                    \n",
    "                    monthly_list = entry.get(\"attributes\", {}).get(list_key, [])\n",
    "                    \n",
    "                    # Add data to the main list and standardize to have the 'dataset' for tracking\n",
    "                    for record in monthly_list:\n",
    "                         record['dataset'] = dataset_name\n",
    "                         all_data.append(record)\n",
    "\n",
    "            else:\n",
    "                print(f\"  -> Failed to retrieve data for {start_date} to {end_date}: HTTP {response.status_code}\")\n",
    "                \n",
    "        print(f\"  -> Finished processing {year}.\")\n",
    "\n",
    "    if all_data:\n",
    "        # Define the mapping from camelCase (API fields) to snake_case (desired schema)\n",
    "        column_mapping = {\n",
    "            'priceArea': 'pricearea',\n",
    "            'productionGroup': 'productiongroup',\n",
    "            'consumptionGroup': 'consumptiongroup', \n",
    "            'meteringPointCount': 'meteringpointcount',\n",
    "            'startTime': 'starttime',\n",
    "            'endTime': 'endtime',\n",
    "            'quantityKwh': 'quantitykwh',\n",
    "            'lastUpdatedTime': 'lastupdatedtime'\n",
    "        }\n",
    "        \n",
    "        # Convert the collected list of dictionaries to a Pandas DataFrame\n",
    "        final_df = pd.DataFrame(all_data)\n",
    "        \n",
    "        # Rename columns directly to snake_case for consistency (This is the key change)\n",
    "        # The 'dataset' column, added earlier, remains as is.\n",
    "        final_df.rename(columns=column_mapping, inplace=True)\n",
    "        \n",
    "        print(f\"\\nTotal data points fetched for {dataset_name}: {len(final_df)}\")\n",
    "        return final_df\n",
    "        \n",
    "    print(f\"\\nNo data successfully fetched for {dataset_name}.\")\n",
    "    return pd.DataFrame()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.3 Data Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting fetch for Dataset: PRODUCTION_PER_GROUP_MBA_HOUR (Years: [2022, 2023, 2024]) ---\n",
      "Expecting data under JSON key: 'productionPerGroupMbaHour'\n",
      "Fetching data for year 2022...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  -> Finished processing 2022.\n",
      "Fetching data for year 2023...\n",
      "  -> Finished processing 2023.\n",
      "Fetching data for year 2024...\n",
      "  -> Finished processing 2024.\n",
      "\n",
      "Total data points fetched for PRODUCTION_PER_GROUP_MBA_HOUR: 657600\n",
      "\n",
      "--- Starting fetch for Dataset: CONSUMPTION_PER_GROUP_MBA_HOUR (Years: [2021, 2022, 2023, 2024]) ---\n",
      "Expecting data under JSON key: 'consumptionPerGroupMbaHour'\n",
      "Fetching data for year 2021...\n",
      "  -> Finished processing 2021.\n",
      "Fetching data for year 2022...\n",
      "  -> Finished processing 2022.\n",
      "Fetching data for year 2023...\n",
      "  -> Finished processing 2023.\n",
      "Fetching data for year 2024...\n",
      "  -> Finished processing 2024.\n",
      "\n",
      "Total data points fetched for CONSUMPTION_PER_GROUP_MBA_HOUR: 876600\n",
      "\n",
      "Production Data Head:\n",
      "                     endtime            lastupdatedtime pricearea  \\\n",
      "0  2022-01-01T01:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "1  2022-01-01T02:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "2  2022-01-01T03:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "3  2022-01-01T04:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "4  2022-01-01T05:00:00+01:00  2025-02-01T18:02:57+01:00       NO1   \n",
      "\n",
      "  productiongroup  quantitykwh                  starttime  \\\n",
      "0           hydro    1291422.4  2022-01-01T00:00:00+01:00   \n",
      "1           hydro    1246209.4  2022-01-01T01:00:00+01:00   \n",
      "2           hydro    1271757.0  2022-01-01T02:00:00+01:00   \n",
      "3           hydro    1204251.8  2022-01-01T03:00:00+01:00   \n",
      "4           hydro    1202086.9  2022-01-01T04:00:00+01:00   \n",
      "\n",
      "                         dataset  \n",
      "0  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "1  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "2  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "3  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "4  PRODUCTION_PER_GROUP_MBA_HOUR  \n",
      "\n",
      "Consumption Data Head:\n",
      "  consumptiongroup                    endtime            lastupdatedtime  \\\n",
      "0            cabin  2021-01-01T01:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "1            cabin  2021-01-01T02:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "2            cabin  2021-01-01T03:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "3            cabin  2021-01-01T04:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "4            cabin  2021-01-01T05:00:00+01:00  2024-12-20T10:35:40+01:00   \n",
      "\n",
      "   meteringpointcount pricearea  quantitykwh                  starttime  \\\n",
      "0              100607       NO1    177071.56  2021-01-01T00:00:00+01:00   \n",
      "1              100607       NO1    171335.12  2021-01-01T01:00:00+01:00   \n",
      "2              100607       NO1    164912.02  2021-01-01T02:00:00+01:00   \n",
      "3              100607       NO1    160265.77  2021-01-01T03:00:00+01:00   \n",
      "4              100607       NO1    159828.69  2021-01-01T04:00:00+01:00   \n",
      "\n",
      "                          dataset  \n",
      "0  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "1  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "2  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "3  CONSUMPTION_PER_GROUP_MBA_HOUR  \n",
      "4  CONSUMPTION_PER_GROUP_MBA_HOUR  \n"
     ]
    }
   ],
   "source": [
    "# Retrieve Production Data (2022-2024)\n",
    "production_df_raw = fetch_data_from_elhub(\n",
    "    dataset_name=PRODUCTION_DATASET,\n",
    "    years=PRODUCTION_YEARS\n",
    ")\n",
    "\n",
    "# Retrieve Consumption Data (2021-2024)\n",
    "consumption_df_raw = fetch_data_from_elhub(\n",
    "    dataset_name=CONSUMPTION_DATASET,\n",
    "    years=CONSUMPTION_YEARS\n",
    ")\n",
    "\n",
    "print(\"\\nProduction Data Head:\")\n",
    "print(production_df_raw.head())\n",
    "print(\"\\nConsumption Data Head:\")\n",
    "print(consumption_df_raw.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3.4 Restart/Refresh Spark Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Refreshing Spark Session for Data Transformation ---\n",
      "Previous Spark session stopped.\n",
      "New Spark session started successfully for transformation.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Refreshing Spark Session for Data Transformation ---\")\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"Previous Spark session stopped.\")\n",
    "except:\n",
    "    pass # Ignore if it was already stopped\n",
    "\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"IND320_A4_Data_Ingestion\")\n",
    "    .config(\"spark.jars.packages\", \"com.datastax.spark:spark-cassandra-connector_2.12:3.5.1\")\n",
    "    .config(\"spark.cassandra.connection.host\", \"127.0.0.1\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "print(\"New Spark session started successfully for transformation.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Spark Transformation and Cassandra Ingestion\n",
    "\n",
    "#### 4.1 Spark Transformation Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Warning: Could not read 2021 data from 'production_2021'. Using only new data.\n",
      "Spark DataFrames created for production (full 2021-2024) and consumption (2021-2024).\n",
      "\n",
      "Production Full Spark DF Schema:\n",
      "root\n",
      " |-- productiongroup: string (nullable = true)\n",
      " |-- pricearea: string (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- endtime: timestamp (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      " |-- lastupdatedtime: timestamp (nullable = true)\n",
      "\n",
      "\n",
      "Consumption Spark DF Schema:\n",
      "root\n",
      " |-- consumptiongroup: string (nullable = true)\n",
      " |-- pricearea: string (nullable = true)\n",
      " |-- starttime: timestamp (nullable = true)\n",
      " |-- endtime: timestamp (nullable = true)\n",
      " |-- quantitykwh: double (nullable = true)\n",
      " |-- lastupdatedtime: timestamp (nullable = true)\n",
      " |-- meteringpointcount: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def transform_pandas_to_spark_df(pandas_df, spark_session, data_type):\n",
    "    \"\"\"\n",
    "    Converts Pandas (which is already clean from the fetch step) to Spark,\n",
    "    and casts time columns to Spark Timestamp type.\n",
    "    \"\"\"\n",
    "    if pandas_df.empty:\n",
    "        print(f\"Warning: Empty DataFrame provided for transformation ({data_type}). Returning empty Spark DF.\")\n",
    "        return spark_session.createDataFrame([], schema=None)\n",
    "        \n",
    "    spark_df = spark_session.createDataFrame(pandas_df)\n",
    "    \n",
    "    # Convert time columns using the timezone-aware format string\n",
    "    time_columns = ['starttime', 'endtime', 'lastupdatedtime']\n",
    "    for col_name in time_columns:\n",
    "        if col_name in spark_df.columns:\n",
    "             spark_df = spark_df.withColumn(\n",
    "                 col_name, \n",
    "                 to_timestamp(col(col_name), \"yyyy-MM-dd'T'HH:mm:ssXXX\") \n",
    "               )\n",
    "    \n",
    "    # Select final columns based on data type for the respective Cassandra tables\n",
    "    # NOTE: meteringpointcount is included here now.\n",
    "    base_cols = ['pricearea', 'starttime', 'endtime', 'quantitykwh', 'lastupdatedtime', 'meteringpointcount']\n",
    "    \n",
    "    if data_type == 'production':\n",
    "        # Production data needs productiongroup\n",
    "        final_cols = ['productiongroup'] + base_cols\n",
    "    else: # Consumption data\n",
    "        # Consumption data needs consumptiongroup\n",
    "        final_cols = ['consumptiongroup'] + base_cols\n",
    "        \n",
    "    # Filter columns to only include those present in the DataFrame\n",
    "    return spark_df.select([c for c in final_cols if c in spark_df.columns])\n",
    "\n",
    "# 1. Transform the raw dataframes (New data: 2022-2024 Production, 2021-2024 Consumption)\n",
    "production_spark_df_new = transform_pandas_to_spark_df(production_df_raw, spark, 'production')\n",
    "consumption_spark_df = transform_pandas_to_spark_df(consumption_df_raw, spark, 'consumption')\n",
    "\n",
    "\n",
    "\n",
    "# 2. CONSOLIDATION: Read the old 2021 Production data from the staging table\n",
    "old_production_table = \"production_2021\"\n",
    "try:\n",
    "    production_spark_df_2021 = (\n",
    "        spark.read.format(\"org.apache.spark.sql.cassandra\")\n",
    "        .options(keyspace=keyspace_name, table=old_production_table)\n",
    "        .load()\n",
    "    )\n",
    "    # Ensure columns match for union (e.g., lowercase names)\n",
    "    production_spark_df_2021 = production_spark_df_2021.select(production_spark_df_new.columns)\n",
    "    print(f\"\\nRead {production_spark_df_2021.count()} 2021 records from '{old_production_table}'.\")\n",
    "\n",
    "    # 3. UNION: Combine the old 2021 data with the new 2022-2024 data\n",
    "    production_spark_df_full = production_spark_df_2021.union(production_spark_df_new)\n",
    "    print(f\"Total Combined Production Records (2021-2024): {production_spark_df_full.count()}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    # If the old table doesn't exist, assume we're only writing the new data (this shouldn't happen)\n",
    "    print(f\"\\nWarning: Could not read 2021 data from '{old_production_table}'. Using only new data.\")\n",
    "    production_spark_df_full = production_spark_df_new\n",
    "\n",
    "print(\"Spark DataFrames created for production (full 2021-2024) and consumption (2021-2024).\")\n",
    "print(\"\\nProduction Full Spark DF Schema:\")\n",
    "production_spark_df_full.printSchema()\n",
    "print(\"\\nConsumption Spark DF Schema:\")\n",
    "consumption_spark_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2 Cassandra Table Creation (New/Updated Tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Target Cassandra tables 'production_hourly' and 'consumption_hourly' explicitly dropped and re-created with the correct schema, including all new fields.\n"
     ]
    }
   ],
   "source": [
    "#### 4.2 Cassandra Table Creation (New/Updated Tables)\n",
    "production_table_name = \"production_hourly\"\n",
    "consumption_table_name = \"consumption_hourly\"\n",
    "\n",
    "# --- CRITICAL FIX: Explicitly drop old tables to ensure the new schema is used ---\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.production_2021;\")\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.{production_table_name};\")\n",
    "session.execute(f\"DROP TABLE IF EXISTS {keyspace_name}.{consumption_table_name};\")\n",
    "# ----------------------------------------------------------------------------------\n",
    "\n",
    "\n",
    "# 3. Production Table: Re-create with the all-lowercase schema (Includes meteringpointcount)\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {keyspace_name}.{production_table_name} (\n",
    "    pricearea text,\n",
    "    productiongroup text,\n",
    "    starttime timestamp,\n",
    "    endtime timestamp,\n",
    "    quantitykwh double,\n",
    "    lastupdatedtime timestamp,\n",
    "    meteringpointcount int,\n",
    "    PRIMARY KEY ((pricearea, productiongroup), starttime)\n",
    ");\n",
    "\"\"\")\n",
    "\n",
    "# 4. Consumption Table: Re-create with the all-lowercase schema (Includes meteringpointcount)\n",
    "session.execute(f\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS {keyspace_name}.{consumption_table_name} (\n",
    "    pricearea text,\n",
    "    consumptiongroup text,\n",
    "    starttime timestamp,\n",
    "    endtime timestamp,\n",
    "    quantitykwh double,\n",
    "    lastupdatedtime timestamp,\n",
    "    meteringpointcount int,\n",
    "    PRIMARY KEY ((pricearea, consumptiongroup), starttime)\n",
    ");\n",
    "\"\"\")\n",
    "print(f\"Target Cassandra tables '{production_table_name}' and '{consumption_table_name}' explicitly dropped and re-created with the correct schema, including all new fields.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.3 Ingestion into Cassandra (Append vs. New Table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:37:11 WARN TaskSetManager: Stage 0 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:37:18 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 0 (TID 0): Attempting to kill Python Worker\n",
      "25/11/27 13:37:18 WARN TaskSetManager: Stage 1 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:37:41 WARN TaskSetManager: Stage 2 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:37:44 WARN TaskSetManager: Stage 5 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Successfully OVERWROTE 657600 records (2021-2024) into 'production_hourly'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:37:48 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 5 (TID 18): Attempting to kill Python Worker\n",
      "25/11/27 13:37:49 WARN TaskSetManager: Stage 6 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:38:17 WARN TaskSetManager: Stage 7 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n",
      "[Stage 7:>                                                          (0 + 8) / 8]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully INSERTED 876600 records (2021-2024) into new table 'consumption_hourly'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#### 4.3 Ingestion into Cassandra (Full Dataset Write)\n",
    "# Ingest Production Data (2021-2024 Full Dataset) - OVERWRITING\n",
    "# We use 'overwrite' because the full, combined dataset is ready and the table was just cleared.\n",
    "if 'production_spark_df_full' in locals() and not production_spark_df_full.isEmpty():\n",
    "    # *** CRITICAL FIX: Add 'confirm.truncate' option to allow overwrite mode ***\n",
    "    production_spark_df_full.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace_name, table=production_table_name, **{'confirm.truncate': True}) \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .save()\n",
    "    print(f\"\\nSuccessfully OVERWROTE {production_spark_df_full.count()} records (2021-2024) into '{production_table_name}'.\")\n",
    "else:\n",
    "    print(f\"\\nSkipped Production Ingestion: No 2021-2024 production data to insert.\")\n",
    "\n",
    "\n",
    "# Ingest Consumption Data (2021-2024) - INSERTING (New table)\n",
    "# The consumption_spark_df contains the full 2021-2024 dataset, so we just write it.\n",
    "if not consumption_spark_df.isEmpty():\n",
    "    consumption_spark_df.write\\\n",
    "        .format(\"org.apache.spark.sql.cassandra\") \\\n",
    "        .options(keyspace=keyspace_name, table=consumption_table_name) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    print(f\"Successfully INSERTED {consumption_spark_df.count()} records (2021-2024) into new table '{consumption_table_name}'.\")\n",
    "else:\n",
    "    print(f\"Skipped Consumption Ingestion: No 2021-2024 consumption data to insert.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. MongoDB Ingestion and Final Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Connected to MongoDB database 'elhub_db'.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:38:21 WARN TaskSetManager: Stage 10 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:38:25 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 10 (TID 36): Attempting to kill Python Worker\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ingestion for Production into collection 'production' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:38:27 WARN TaskSetManager: Stage 11 contains a task of very large size (8991 KiB). The maximum recommended task size is 1000 KiB.\n",
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing documents in MongoDB collection 'production'.\n",
      "Inserted 657600 Production records.\n",
      "Total documents in collection: 657600\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/11/27 13:46:08 WARN TaskSetManager: Stage 12 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n",
      "25/11/27 13:46:12 WARN PythonRunner: Detected deadlock while completing task 0.0 in stage 12 (TID 45): Attempting to kill Python Worker\n",
      "25/11/27 13:46:12 WARN TaskSetManager: Stage 13 contains a task of very large size (12677 KiB). The maximum recommended task size is 1000 KiB.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting Ingestion for Consumption into collection 'consumption_hourly' ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleared existing documents in MongoDB collection 'consumption_hourly'.\n",
      "Inserted 876600 Consumption records.\n",
      "Total documents in collection: 876600\n",
      "\n",
      "Spark session stopped. All ingestion complete.\n"
     ]
    }
   ],
   "source": [
    "### 5. MongoDB Ingestion and Final Cleanup\n",
    "\n",
    "# 5.1 Connect to MongoDB\n",
    "if not MONGO_URI:\n",
    "    print(\"\\nMongoDB ingestion skipped (MONGO_URI not loaded).\")\n",
    "else:\n",
    "    client = MongoClient(MONGO_URI)\n",
    "    db = client[MONGO_DB]\n",
    "    print(f\"\\nConnected to MongoDB database '{MONGO_DB}'.\")\n",
    "\n",
    "    def ingest_to_mongo(spark_df, collection_name, data_type_label):\n",
    "        \"\"\"Helper function to convert Spark DF to Pandas and ingest into a specific MongoDB collection.\"\"\"\n",
    "        if spark_df.isEmpty():\n",
    "            print(f\"Skipped {data_type_label} Ingestion: Spark DataFrame is empty.\")\n",
    "            return\n",
    "\n",
    "        # Get the target collection instance\n",
    "        collection = db[collection_name]\n",
    "        print(f\"\\n--- Starting Ingestion for {data_type_label} into collection '{collection_name}' ---\")\n",
    "        \n",
    "        # Convert Spark DataFrame back to Pandas\n",
    "        mongo_pandas_df = spark_df.toPandas()\n",
    "        \n",
    "        # Convert time columns to Python datetime objects for BSON compliance\n",
    "        for col_name in ['starttime', 'endtime', 'lastupdatedtime']:\n",
    "            if col_name in mongo_pandas_df.columns:\n",
    "                mongo_pandas_df[col_name] = mongo_pandas_df[col_name].apply(\n",
    "                    lambda x: x.to_pydatetime() if pd.notna(x) else None\n",
    "                )\n",
    "                \n",
    "        # Convert to a list of dictionaries (BSON documents)\n",
    "        records = mongo_pandas_df.to_dict(\"records\")\n",
    "\n",
    "        # Clear existing data and insert the new data\n",
    "        collection.delete_many({})\n",
    "        print(f\"Cleared existing documents in MongoDB collection '{collection_name}'.\")\n",
    "        \n",
    "        # Insert the new data\n",
    "        result = collection.insert_many(records)\n",
    "        print(f\"Inserted {len(result.inserted_ids)} {data_type_label} records.\")\n",
    "        print(f\"Total documents in collection: {collection.count_documents({})}\")\n",
    "        \n",
    "    # --- INGESTION 1: PRODUCTION DATA ---\n",
    "    # Writes the production data to the MONGO_COLLECTION (e.g., \"production\")\n",
    "    ingest_to_mongo(production_spark_df_full, MONGO_COLLECTION, \"Production\")\n",
    "\n",
    "    # --- INGESTION 2: CONSUMPTION DATA ---\n",
    "    # Writes the consumption data to the new MONGO_CONSUMPTION_COLLECTION (e.g., \"consumption_hourly\")\n",
    "    ingest_to_mongo(consumption_spark_df, MONGO_CONSUMPTION_COLLECTION, \"Consumption\")\n",
    "\n",
    "\n",
    "### 6. Final Cleanup\n",
    "# Stop Spark Session\n",
    "try:\n",
    "    spark.stop()\n",
    "    print(\"\\nSpark session stopped. All ingestion complete.\")\n",
    "except Exception as e:\n",
    "    print(f\"\\nSpark session stop failed (was likely already stopped): {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log\n",
    "This final project has been very demanding. I usually expect a lot from myself, but this has truly been exhausting. I began working on it last week just by experimenting a little, and ended up spending all hours on it throughout the week.\n",
    "\n",
    "The first major challenge was reading the data into MongoDB. When I finally thought I had finished the notebook, it ended up crashing my entire page before I had even modified anything. I spent two full days trying to find the root cause—getting closer, but never quite close enough. The issue turned out to be related to collections and incorrectly referenced groups, which the page was unable to handle properly.\n",
    "\n",
    "As I continued developing, more issues appeared, most of them tracing back to small mistakes in the data ingestion step. These took an enormous amount of time to identify and correct. Even after fixing the main problem, I kept getting new errors, which I later discovered were caused simply by caching. I faced many obstacles, learned a lot, and often felt frustrated. I really tried to understand and solve things myself rather than relying on AI for everything—and that definitely takes more time. I also learned that AI doesn’t always give the right answer, so I had to work things out on my own. I faced numerous errors while combining datasets and building the pipelines.\n",
    "\n",
    "During the analysis phase, the SWC analysis showed that colder temperatures lead to higher energy consumption, higher wind speeds increase production, and using bigger time windows produces more stable forecasts.\n",
    "\n",
    "If I had more time, I would revisit the SARIMAX model. I’m not convinced it’s correct. I spent hours trying to get it to work without success and eventually received help from peers. I wanted to personalize that part more, but the logic wasn’t easily transferable. No matter how many hours I put in, I still couldn’t get it right.\n",
    "\n",
    "With more time, I would also refine the aesthetics, clean up comments, and tidy the code. I would also spend more time doing the bonus assignments, as I only had time to do the chaching with spinners, and som error handling.\n",
    " \n",
    "This project has been very challenging. But on the positive side, I have learned a great deal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Usage\n",
    "Gemini, copilot and ChatGPT served as an essential codevelopment and strategic debugging partner to overcome major technical barriers. The AI's key role was resolving complex, recurring indexing and caching failures, debugging database problems, help with devolping code, rewriting my suggestions, ensuring the final analytical structure was robust, compliant, and accelerated the overall project delivery."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "D2D_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
